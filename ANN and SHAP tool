# -*- coding: utf-8 -*-
"""
Created on Wed May 19 20:56:19 2021

@author: MyPC
"""


# -*- coding: utf-8 -*-
"""
Created on Thu Jan 28 21:52:21 2021

@author: MyPC
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.neural_network import MLPRegressor
from sklearn.preprocessing import StandardScaler
import shap
import os

#Change the link
os.chdir("D:\Study\VNP")

#input data
df = pd.read_csv("DataThesis3/train1.csv")
q = pd.read_csv("DataThesis3/qquarterly.csv")
m =pd.read_csv("DataThesis3/qmonthly.csv")
w =pd.read_csv("DataThesis3/qweekly.csv")

#create countries dummies
dummies = pd.get_dummies(df["countries"])
df = df.join(dummies)
dummies = pd.get_dummies(m["countries"])
m = m.join(dummies).dropna()
dummies =  pd.get_dummies(w["countries"])
w = w.join(dummies).dropna()
w["date"] = pd.DatetimeIndex(w["date"], dayfirst=True)
dummies =  pd.get_dummies(q["countries"])
q = q.join(dummies).dropna()



#take train data
df = df.set_index(["date", "countries"])
q = q.set_index(["date", "countries"])
m = m.set_index(["date", "countries"])
w = w.set_index(["date", "countries"])

#standard scaler training data
X = df[df.columns[1:]]
y = df[df.columns[0]]

#Create ANN
network = MLPRegressor(hidden_layer_sizes=(300, 10),
  solver = "adam",
  activation = "relu",
  learning_rate_init = 0.001,
  tol=1e-4,
  max_iter=7000,
  early_stopping = True,
  random_state=0)

#run Ensemble
from sklearn.base import BaseEstimator, TransformerMixin
import numpy as np
from copy import deepcopy


class Ensemble(BaseEstimator, TransformerMixin):

    def __init__(self, learner, size_ensemble=100):
        self.learner=learner
        self.size_ensemble=size_ensemble

    def fit(self, X, y=None):
        self.weak_learners = []
        for i in range(self.size_ensemble):
            weak_learner = deepcopy(self.learner)
            weak_learner.set_params(**{'random_state':i*1000})
            self.weak_learners.append(weak_learner)
        
        #fit ensemble
        self.weak_learners = [wl.fit(X, y) for wl in self.weak_learners]

        return self
    
    def predict(self, X):
        self.predictions = np.stack([wl.predict(X) for wl in self.weak_learners])
        return np.mean(self.predictions, axis=0)

mlp = Ensemble(learner = network, size_ensemble = 5)

# Spittling the dataset into the Training set and Test set
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2, random_state=0)

#fit model
history=mlp.fit(X, y)

#predict
qy_pred = history.predict(q)
my_pred = history.predict(m)
wy_pred = history.predict(w)

#Shapvalues
#Quarterly Shapley values
qX_summary = shap.kmeans(X, 10)
qexplainer = shap.KernelExplainer(history.predict, qX_summary)
qshap_values = qexplainer.shap_values(X)
shap.summary_plot(qshap_values, X)


#Monthly Shapvalues
mX_summary = shap.kmeans(m, 10)
mexplainer = shap.KernelExplainer(history.predict, mX_summary)
mshap_values = mexplainer.shap_values(m)
shap.summary_plot(mshap_values, m)

#Weekly Shapvalues
wX_summary = shap.kmeans(w, 10)
wexplainer = shap.KernelExplainer(history.predict, wX_summary)
wshap_values = wexplainer.shap_values(w)
shap.summary_plot(wshap_values, w)

#Weekly pandemic Shapvalues
wpad = w.reset_index()
wpad = wpad.loc[(wpad["date"]>"2020")].set_index(["date","countries"])
wpadX_summary = shap.kmeans(wpad, 10)
wpadexplainer = shap.KernelExplainer(history.predict, wpadX_summary)
wpadshap_values = wpadexplainer.shap_values(wpad)
shap.summary_plot(wpadshap_values, wpad)
